version: '3'
services:

  # # https://github.com/encode/broadcaster#available-backends
  broadcast_channel:
    image: postgres:13
    environment:
      - POSTGRES_DB=postgres
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
    networks:
      - mynetwork
  opal_server:
    # by default we run opal-server from latest official image
    image: permitio/opal-server:latest
    env_file:
      - .env
    environment:
      # the broadcast backbone uri used by opal server workers (see comments above for: broadcast_channel)
      - OPAL_BROADCAST_URI=postgres://postgres:postgres@broadcast_channel:5432/postgres
      # number of uvicorn workers to run inside the opal-server container
      - UVICORN_NUM_WORKERS=4
      # the git repo hosting our policy
      # - if this repo is not public, you can pass an ssh key via `OPAL_POLICY_REPO_SSH_KEY`)
      # - the repo we pass in this example is *public* and acts as an example repo with dummy rego policy
      # - for more info, see: https://docs.opal.ac/tutorials/track_a_git_repo
      - OPAL_POLICY_REPO_URL=https://github.com/nadavgross/trino-opa-example
      # in this example we will use a polling interval of 30 seconds to check for new policy updates (git commits affecting the rego policy).
      # however, it is better to utilize a git *webhook* to trigger the server to check for changes only when the repo has new commits.
      # for more info see: https://docs.opal.ac/tutorials/track_a_git_repo
      - OPAL_POLICY_REPO_POLLING_INTERVAL=30
      # configures from where the opal client should initially fetch data (when it first goes up, after disconnection, etc).
      # the data sources represents from where the opal clients should get a "complete picture" of the data they need.
      # after the initial sources are fetched, the client will subscribe only to update notifications sent by the server.
      # - OPAL_DATA_CONFIG_SOURCES={"config":{"entries":[{"url":"http://opal_server:7002/policy-data","topics":["policy_data"],"dst_path":"/static"}]}}
      - OPAL_LOG_FORMAT_INCLUDE_PID=true
    ports:
      # exposes opal server on the host machine, you can access the server at: http://localhost:7002
      - "7002:7002"
    depends_on:
      - broadcast_channel
    networks:
      - mynetwork
  opa:
    # by default we run opal-client from latest official image
    image: permitio/opal-client:latest
    environment:
      - OPAL_SERVER_URL=http://opal_server:7002
      - OPAL_LOG_FORMAT_INCLUDE_PID=true
      - OPAL_INLINE_OPA_LOG_FORMAT=http

      # Uncomment the following lines to enable storing & loading OPA data from a backup file:
      # - OPAL_OFFLINE_MODE_ENABLED=true
    # volumes:
    #  - opa_backup:/opal/backup:rw

    ports:
      # exposes opal client on the host machine, you can access the client at: http://localhost:7766
      - "7766:7000"
      # exposes the OPA agent (being run by OPAL) on the host machine
      # you can access the OPA api that you know and love at: http://localhost:8181
      # OPA api docs are at: https://www.openpolicyagent.org/docs/latest/rest-api/
      - "8181:8181"
    depends_on:
      - opal_server
    # this command is not necessary when deploying OPAL for real, it is simply a trick for dev environments
    # to make sure that opal-server is already up before starting the client.
    command: sh -c "exec ./wait-for.sh opal_server:7002 --timeout=60 -- ./start.sh"
    networks:
      - mynetwork

  trino:
    build:
      context: ./trino
      dockerfile: Dockerfile
    image: trino_plugin:latest
    depends_on:
      - postgres
      - minio
    ports:
      - "8080:8080"
    networks:
      - mynetwork

  # opa:
  #   build:
  #     context: ./opa
  #     dockerfile: Dockerfile
  #   image: opa:latest
  #   ports:
  #     - "8182:8182"
  #   networks:
  #     - mynetwork

  abac_api:
    build:
      context: ./abac_api
      dockerfile: Dockerfile
    image: abac_api:latest
    ports:
      - "8081:8081"
    networks:
      - mynetwork

  attribute_db:
    image: mongo:latest
    ports:
      - "27017:27017"
    networks:
      - mynetwork

  setup:
    build:
      context: ./
      dockerfile: ./setup/Dockerfile
    image: setup:latest
    restart: "no"
    networks:
      - mynetwork
    depends_on:
      - attribute_db
      - trino

  rest:
    image: tabulario/iceberg-rest
    container_name: iceberg-rest
    networks:
      - mynetwork
    depends_on:
      postgres:
        condition: service_healthy
    ports:
      - 8183:8183
    environment:
      AWS_ACCESS_KEY_ID: minio
      AWS_SECRET_ACCESS_KEY: minio123
      AWS_REGION: us-east-1
      CATALOG_WAREHOUSE: s3://datalake/
      CATALOG_IO__IMPL: org.apache.iceberg.aws.s3.S3FileIO
      CATALOG_S3_ENDPOINT: http://minio:9000
      CATALOG_URI: jdbc:postgresql://postgres/datalake_catalog
      CATALOG_JDBC_USER: admin
      CATALOG_JDBC_PASSWORD: password
      REST_PORT: 8183

  postgres:
    image: postgres:13
    container_name: postgres
    environment:
      PGDATA: /var/lib/postgresql/data
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: password
      POSTGRES_DB: datalake_catalog
      POSTGRES_HOST_AUTH_METHOD: md5
    networks:
      - mynetwork
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U admin -d datalake_catalog" ]
      interval: 5s
      timeout: 5s
      retries: 5

  minio:
    image: 'minio/minio'
    container_name: minio
    ports:
      - '9000:9000'
      - '9001:9001'
    volumes:
      - minio_data:/data
    environment:
      MINIO_ROOT_USER: minio
      MINIO_ROOT_PASSWORD: minio123
      MINIO_DOMAIN: minio
    command: [ "server", "/data", "--console-address", ":9001" ]
    networks:
      mynetwork:
        aliases:
          - datalake.minio

  mc:
    depends_on:
      - minio
    image: minio/mc
    container_name: mc
    networks:
      - mynetwork
    environment:
      AWS_ACCESS_KEY_ID: minio
      AWS_SECRET_ACCESS_KEY: minio123
      AWS_REGION: us-east-1
    entrypoint: >
      /bin/sh -c "
          until (/usr/bin/mc config host add minio http://minio:9000 minio minio123) do echo '...waiting...' && sleep 1; done;
          /usr/bin/mc mb minio/datalake;
          /usr/bin/mc policy set public minio/datalake;
          tail -f /dev/null
      "  

volumes:
  minio_data:
    driver: local
  postgres_data:
    driver: local
  opa_backup:

networks:
  mynetwork:
    driver: bridge
